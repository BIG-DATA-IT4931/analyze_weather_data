{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.conf import SparkConf\n",
    "from pyspark.context import SparkContext\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import scipy.ndimage as ndimage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkConf\n",
    "credentials_location = '/home/phuc_0703/learnDE/data-engineering-zoomcamp/strong-ward-437213-j6-c3ae16d10e5f.json'\n",
    "\n",
    "conf = SparkConf() \\\n",
    "    .setMaster('local[*]') \\\n",
    "    .setAppName('test') \\\n",
    "    .set(\"spark.jars\", \"/home/phuc_0703/learnDE/data-engineering-zoomcamp/05-batch/libs/gcs-connector.jar,/home/phuc_0703/learnDE/data-engineering-zoomcamp/05-batch/libs/bigquery-connector.jar\") \\\n",
    "    .set(\"spark.hadoop.google.cloud.auth.service.account.enable\", \"true\") \\\n",
    "    .set(\"spark.hadoop.google.cloud.auth.service.account.json.keyfile\", credentials_location)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "sc = SparkContext(conf=conf)\n",
    "\n",
    "hadoop_conf = sc._jsc.hadoopConfiguration()\n",
    "\n",
    "hadoop_conf.set(\"fs.AbstractFileSystem.gs.impl\",  \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS\")\n",
    "hadoop_conf.set(\"fs.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem\")\n",
    "hadoop_conf.set(\"fs.gs.auth.service.account.json.keyfile\", credentials_location)\n",
    "hadoop_conf.set(\"fs.gs.auth.service.account.enable\", \"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .config(conf=sc.getConf()) \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "df_weather_main = (\n",
    "    spark.read.option(\"timestampAsString\", \"true\")\n",
    "    .parquet(\"gs://weather_bigdata_20241/other_data_transformed/*\")\n",
    ")\n",
    "# df_weather_main = df_weather_main.withColumn(\"time\", df_weather_main[\"time\"].cast(\"timestamp\"))\n",
    "# df_weather_main = df_weather_main.withColumn(\"sst\", df_weather_main[\"sst\"] - 273.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "df_weather_main.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "df_weather_main.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "storm_count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "def distance_matrix(lons,lats):\n",
    "    '''Calculates the distances (in km) between any two cities based on the formulas\n",
    "    c = sin(lati1)*sin(lati2)+cos(longi1-longi2)*cos(lati1)*cos(lati2)\n",
    "    d = EARTH_RADIUS*Arccos(c)\n",
    "    where EARTH_RADIUS is in km and the angles are in radians.\n",
    "    Source: http://mathforum.org/library/drmath/view/54680.html\n",
    "    This function returns the matrix.'''\n",
    "\n",
    "    EARTH_RADIUS = 6378.1\n",
    "    X = len(lons)\n",
    "    Y = len(lats)\n",
    "    assert X == Y, 'lons and lats must have same number of elements'\n",
    "\n",
    "    d = np.zeros((X,X))\n",
    "\n",
    "    #Populate the matrix.\n",
    "    for i2 in range(len(lons)):\n",
    "        lati2 = lats[i2]\n",
    "        loni2 = lons[i2]\n",
    "        c = np.sin(np.radians(lats)) * np.sin(np.radians(lati2)) + \\\n",
    "            np.cos(np.radians(lons-loni2)) * \\\n",
    "            np.cos(np.radians(lats)) * np.cos(np.radians(lati2))\n",
    "        d[c<1,i2] = EARTH_RADIUS * np.arccos(c[c<1])\n",
    "\n",
    "    return d\n",
    "\n",
    "def diameter(area):\n",
    "    r = np.sqrt((area/np.pi))\n",
    "    return (r*2)\n",
    "\n",
    "def major_ratio(area, ratio):\n",
    "    # given the area and ratio of minor to major axis of an ellipse, returns the length of the major axis\n",
    "    maj = np.sqrt(area/(np.pi*ratio))\n",
    "    return maj \n",
    "\n",
    "def major_ecc(area, ecc):\n",
    "    # given the area and eccentricity of an ellipse, returns the length of the major axis    \n",
    "    denom = np.pi*np.sqrt(1-(ecc**2))\n",
    "    maj = np.sqrt(area/denom)\n",
    "    return maj\n",
    "\n",
    "def ecc(ratio):\n",
    "    #given ratio of a/b, calculates eccentricity of ellipse\n",
    "    e = (ratio)*(np.sqrt((ratio**-2)-1))\n",
    "    return e\n",
    "\n",
    "def nanmean(array, axis=None):\n",
    "    return np.mean(np.ma.masked_array(array, np.isnan(array)), axis)\n",
    "\n",
    "def len_deg_lon(lat):\n",
    "    '''\n",
    "    Returns the length of one degree of longitude (at latitude\n",
    "    specified) in km.\n",
    "    '''\n",
    "\n",
    "    R = 6371. # Radius of Earth [km]\n",
    "\n",
    "    return (np.pi/180.) * R * np.cos( lat * np.pi/180. )\n",
    "\n",
    "def spatial_filter(msl, lon, lat, res, cut_lon, cut_lat):\n",
    "    '''\n",
    "    Performs a spatial filter, removing all features with\n",
    "    wavelenth scales larger than cut_lon in longitude and\n",
    "    cut_lat in latitude from msl (defined in grid given\n",
    "    by lon and lat).  msl has spatial resolution of res\n",
    "    and land identified by np.nan's\n",
    "    '''\n",
    "\n",
    "    msl_filt = np.zeros(msl.shape)\n",
    "\n",
    "    # see Chelton et al, Prog. Ocean., 2011 for explanation of factor of 1/5\n",
    "    sig_lon = (cut_lon/5.) / res\n",
    "    sig_lat = (cut_lat/5.) / res\n",
    "\n",
    "    land = np.isnan(msl)\n",
    "    msl[land] = nanmean(msl)\n",
    "\n",
    "    msl_filt =  ndimage.gaussian_filter(msl, [sig_lat, sig_lon])\n",
    "\n",
    "    msl_filt[land] = np.nan\n",
    "\n",
    "    return msl_filt\n",
    "\n",
    "\n",
    "def detect_storms(msl, wind_speed, lon, lat, res, order, Npix_min, Npix_max, rel_amp_thresh, d_thresh, cyc, cut_lon, cut_lat, globe=False):\n",
    "    '''\n",
    "    Detect storms present in msl which satisfy the criteria.\n",
    "    Algorithm is an adaptation of an eddy detection algorithm,\n",
    "    outlined in Chelton et al., Prog. ocean., 2011, App. B.2,\n",
    "    with modifications needed for storm detection.\n",
    "\n",
    "    msl is a 2D array specified on grid defined by lat and lon.\n",
    "\n",
    "    res is the horizontal grid resolution in degrees of msl\n",
    "\n",
    "    Npix_min is the minimum number of pixels within which an\n",
    "    extremum of msl must lie (recommended: 9).\n",
    "\n",
    "    cyc = 'cyclonic' or 'anticyclonic' specifies type of system\n",
    "    to be detected (cyclonic storm or high-pressure systems)\n",
    "\n",
    "    globe is an option to detect storms on a globe, i.e. with periodic\n",
    "    boundaries in the West/East. Note that if using this option the \n",
    "    supplied longitudes must be positive only (i.e. 0..360 not -180..+180).\n",
    "\n",
    "    Function outputs lon, lat coordinates of detected storms\n",
    "    '''\n",
    "\n",
    "    len_deg_lat = 111.325 # length of 1 degree of latitude [km]\n",
    "\n",
    "    msl = spatial_filter(msl, lon, lat, res, cut_lon, cut_lat)\n",
    "    # Need to repeat global msl to the West and East to properly detect around the edge\n",
    "    if globe:\n",
    "        dl = 20. # Degrees longitude to repeat on East and West of edge\n",
    "        iEast = np.where(lon >= 360. - dl)[0][0]\n",
    "        iWest = np.where(lon <= dl)[0][-1]\n",
    "        lon = np.append(lon[iEast:]-360, np.append(lon, lon[:iWest]+360))\n",
    "        msl = np.append(msl[:,iEast:], np.append(msl, msl[:,:iWest], axis=1), axis=1)\n",
    "\n",
    "    llon, llat = np.meshgrid(lon, lat)\n",
    "\n",
    "    lon_storms = np.array([])\n",
    "    lat_storms = np.array([])\n",
    "    amp_storms = np.array([])\n",
    "    area_storms = np.array([])\n",
    "    max_wind_speeds = np.array([])\n",
    "    regions_storm = []\n",
    "    \n",
    "\n",
    "    # ssh_crits is an array of ssh levels over which to perform storm detection loop\n",
    "    # ssh_crits increasing for 'cyclonic', decreasing for 'anticyclonic'\n",
    "    ssh_crits = np.linspace(np.nanmin(msl), np.nanmax(msl), 200)\n",
    "    ssh_crits = np.array([100000])\n",
    "    if order == 'topdown':\n",
    "        ssh_crits = np.flipud(ssh_crits)\n",
    "\n",
    "    # loop over ssh_crits and remove interior pixels of detected storms from subsequent loop steps\n",
    "    for ssh_crit in ssh_crits:\n",
    " \n",
    "    # 1. Find all regions with eta greater (less than) than ssh_crit for anticyclonic (cyclonic) storms (Chelton et al. 2011, App. B.2, criterion 1)\n",
    "        if cyc == 'anticyclonic':\n",
    "            regions, nregions = ndimage.label( (msl>ssh_crit).astype(int) )\n",
    "        elif cyc == 'cyclonic':\n",
    "            regions, nregions = ndimage.label( (msl<ssh_crit).astype(int) )\n",
    "\n",
    "        for iregion in range(nregions):\n",
    " \n",
    "    # 2. Calculate number of pixels comprising detected region, reject if not within >= Npix_min\n",
    "            region = (regions==iregion+1).astype(int)\n",
    "            region_Npix = region.sum()\n",
    "            storm_area_within_limits = ((region_Npix >= Npix_min) * (region_Npix <= Npix_max))\n",
    " \n",
    "    # 3. Detect presence of local maximum (minimum) for anticylones (cyclones), reject if non-existents\n",
    "            interior = ndimage.binary_erosion(region)\n",
    "            #exterior = region.astype(bool) - interior\n",
    "            exterior = np.logical_xor(region.astype(bool), interior)\n",
    "            if interior.sum() == 0:\n",
    "                continue\n",
    "            if cyc == 'anticyclonic':\n",
    "                has_internal_ext = msl[interior].max() > msl[exterior].max()\n",
    "            elif cyc == 'cyclonic':\n",
    "                has_internal_ext = msl[interior].min() < msl[exterior].min()\n",
    " \n",
    "    # 4. Find amplitude of region, reject if < amp_thresh\n",
    "            if cyc == 'anticyclonic':\n",
    "                amp_abs = msl[interior].max()\n",
    "                amp = amp_abs - msl[exterior].mean()\n",
    "            elif cyc == 'cyclonic':\n",
    "                amp_abs = msl[interior].min()\n",
    "                amp = msl[exterior].mean() - amp_abs\n",
    "            #amp_thresh = np.abs(np.diff(ssh_crits)[0])\n",
    "            is_tall_storm = amp >= rel_amp_thresh\n",
    "\n",
    "    # 5. Find maximum linear dimension of region, reject if < d_thresh\n",
    "            lon_ext = llon[exterior]\n",
    "            lat_ext = llat[exterior]\n",
    "            d = distance_matrix(lon_ext, lat_ext)\n",
    "            is_small_storm = d.max() < d_thresh\n",
    "    \n",
    "\n",
    "    # Quit loop if these are not satisfied\n",
    "            if np.logical_not(storm_area_within_limits * is_tall_storm * is_small_storm):\n",
    "                # print(\"storm_area_within_limits: \" + str(storm_area_within_limits))\n",
    "                # print(\"has_internal_ext: \" + str(has_internal_ext))\n",
    "                # print(\"is_tall_storm: \" + str(is_tall_storm))\n",
    "                # print(\"is_small_storm: \" + str(is_small_storm))\n",
    "                # print(str(storm_area_within_limits * has_internal_ext * is_tall_storm * is_small_storm))\n",
    "                continue\n",
    " \n",
    "    # Detected storms:\n",
    "            if (storm_area_within_limits * is_tall_storm * is_small_storm):\n",
    "                # find centre of mass of storm\n",
    "                storm_object_with_mass = msl * region\n",
    "                storm_object_with_mass[np.isnan(storm_object_with_mass)] = 0\n",
    "                min_pressure_pos = np.unravel_index(np.argmin(msl), msl.shape)\n",
    "    \n",
    "                # Chuyển đổi chỉ số vị trí (min_pressure_pos) thành tọa độ kinh độ và vĩ độ\n",
    "                j_min, i_min = min_pressure_pos\n",
    "                lon_cen = np.interp(i_min, range(0,len(lon)), lon)\n",
    "                lat_cen = np.interp(j_min, range(0,len(lat)), lat)\n",
    "                # Remove storms detected outside global domain (lon < 0, > 360)\n",
    "                #if globe * (lon_cen >= 0.) * (lon_cen <= 360.):\n",
    "                \n",
    "                # Save storm\n",
    "                lon_storms = np.append(lon_storms, lon_cen)\n",
    "                lat_storms = np.append(lat_storms, lat_cen)\n",
    "                \n",
    "                # assign (and calculated) amplitude, area, and scale of storms\n",
    "                amp_storms = np.append(amp_storms, amp_abs)\n",
    "                area = region_Npix * res**2 * len_deg_lat * len_deg_lon(lat_cen) # [km**2]\n",
    "                area_storms = np.append(area_storms, area)                \n",
    "                # remove its interior pixels from further storm detection\n",
    "                storm_mask = np.ones(msl.shape)\n",
    "                storm_mask[interior.astype(int)==1] = np.nan\n",
    "                storm_mask = msl * storm_mask\n",
    "                region_storm = np.isnan(storm_mask).astype(int)\n",
    "                regions_storm.append(region_storm)\n",
    "                max_wind_speed = (wind_speed*region_storm).max()\n",
    "                max_wind_speeds = np.append(max_wind_speeds, max_wind_speed)\n",
    "\n",
    "    return lon_storms, lat_storms, amp_storms, max_wind_speeds, area_storms, regions_storm, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, asc, desc\n",
    "import numpy as np\n",
    "attributes = {\n",
    "    \"U10\": \"u10\",\n",
    "    \"V10\": \"v10\",\n",
    "    \"Mean Sea Level Pressure\": \"msl\",\n",
    "}\n",
    "startTime = \"2022-10-01 00:00:00\"\n",
    "columns = [\"time\", \"latitude\", \"longitude\"] + list(attributes.values())\n",
    "filtered_df = df_weather_main.filter(\n",
    "    (col(\"time\") >= startTime) & (col(\"time\") <= \"2022-10-31 23:00:00\")\n",
    ").select(*columns).orderBy(asc(\"time\"), desc(\"latitude\"), asc(\"longitude\"))\n",
    "data = filtered_df.collect()\n",
    "reshaped_data = {}\n",
    "for attr_name, attr_col in attributes.items():\n",
    "    col_data = [row[attr_col] for row in data]\n",
    "    dim1 = len(col_data) // (65 * 41)\n",
    "    reshaped_data[attr_col] = np.reshape(col_data, (dim1, 65, 41))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import lit\n",
    "from datetime import datetime, timedelta\n",
    "import numpy as np\n",
    "\n",
    "msl = reshaped_data[\"msl\"]\n",
    "lon = np.arange(102, 112.25, 0.25)\n",
    "lat = np.arange(24, 7.75,-0.25)\n",
    "u10 = reshaped_data[\"u10\"]\n",
    "v10 = reshaped_data[\"v10\"]\n",
    "wind_speed = np.sqrt(u10*u10+v10*v10)\n",
    "res = 0.25 \n",
    "order = 'topdown' \n",
    "Npix_min = 9\n",
    "Npix_max = 6000  \n",
    "rel_amp_thresh = 100  \n",
    "d_thresh = 2500  \n",
    "cyc = 'cyclonic'  \n",
    "cut_lon = 1  #\n",
    "cut_lat = 1  \n",
    "globe = False  \n",
    "def haversine_distance(lon1, lat1, lon2, lat2):\n",
    "    return np.sqrt((lon1 - lon2) ** 2 + (lat1 - lat2) ** 2)\n",
    "storm_id = None\n",
    "prev_lon, prev_lat = None, None\n",
    "rows = []\n",
    "for i in range(msl.shape[0]):\n",
    "    time = datetime.strptime(startTime, \"%Y-%m-%d %H:%M:%S\") + timedelta(hours=i)\n",
    "\n",
    "    lon_storms, lat_storms, amp_storms, max_wind_speeds, area_storms, regions_storm = detect_storms(\n",
    "        msl[i, :, :], wind_speed[i, :, :], lon, lat, res, order, Npix_min, Npix_max, rel_amp_thresh, \n",
    "        d_thresh, cyc, cut_lon, cut_lat, globe\n",
    "    )\n",
    "\n",
    "    if lon_storms.size == 0:\n",
    "        continue\n",
    "        # rows.append(Row(\n",
    "        #     time=time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "        #     id=\"2024-00\",\n",
    "        #     lon_storm=None,\n",
    "        #     lat_storm=None,\n",
    "        #     amp_storm=None,\n",
    "        #     max_wind_speed=None,\n",
    "        #     area_storm=None,\n",
    "        #     regions_storm=None\n",
    "        # ))\n",
    "    else:\n",
    "        lon_storm = lon_storms[0]\n",
    "        lat_storm = lat_storms[0]\n",
    "        amp_storm = amp_storms[0]\n",
    "        max_wind_speed = max_wind_speeds[0]\n",
    "        area_storm = area_storms[0]\n",
    "        regions_storm = np.array(regions_storm)\n",
    "        regions_shape = regions_storm.shape\n",
    "        regions_storm_str = f\"{regions_shape}_{regions_storm.flatten().tolist()}\"\n",
    "\n",
    "        if prev_lon is not None and haversine_distance(lon_storm, lat_storm, prev_lon, prev_lat) < 1.5:\n",
    "            storm_id = f\"2022-{storm_count:02d}\"\n",
    "        else:\n",
    "            storm_count += 1\n",
    "            storm_id = f\"2022-{storm_count:02d}\"\n",
    "\n",
    "        prev_lon, prev_lat = lon_storm, lat_storm\n",
    "\n",
    "        rows.append(Row(\n",
    "            time=time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "            id=storm_id,\n",
    "            lon_storm=lon_storm,\n",
    "            lat_storm=lat_storm,\n",
    "            amp_storm=amp_storm,\n",
    "            max_wind_speed=max_wind_speed,\n",
    "            area_storm=area_storm,\n",
    "            regions_storm=regions_storm_str\n",
    "        ))\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, TimestampType\n",
    "\n",
    "# Định nghĩa schema với trường time là TimestampType\n",
    "schema = StructType([\n",
    "    StructField(\"time\", TimestampType(), True),  # TimestampType cho trường time\n",
    "    StructField(\"id\", StringType(), True),\n",
    "    StructField(\"lon_storm\", DoubleType(), True),\n",
    "    StructField(\"lat_storm\", DoubleType(), True),\n",
    "    StructField(\"amp_storm\", DoubleType(), True),\n",
    "    StructField(\"max_wind_speed\", DoubleType(), True),\n",
    "    StructField(\"area_storm\", DoubleType(), True),\n",
    "    StructField(\"regions_storm\", StringType(), True)\n",
    "])\n",
    "\n",
    "rows_with_converted_time = [\n",
    "    Row(\n",
    "        time=datetime.strptime(row.time, \"%Y-%m-%d %H:%M:%S\"),\n",
    "        id=row.id,\n",
    "        lon_storm=float(row.lon_storm) if row.lon_storm is not None else None,\n",
    "        lat_storm=float(row.lat_storm) if row.lat_storm is not None else None,\n",
    "        amp_storm=float(row.amp_storm) if row.amp_storm is not None else None,\n",
    "        max_wind_speed=float(row.max_wind_speed) if row.max_wind_speed is not None else None,\n",
    "        area_storm=float(row.area_storm) if row.area_storm is not None else None,\n",
    "        regions_storm=row.regions_storm\n",
    "    )\n",
    "    for row in rows\n",
    "]\n",
    "\n",
    "\n",
    "# Định nghĩa schema với trường time là TimestampType\n",
    "schema = StructType([\n",
    "    StructField(\"time\", TimestampType(), True),  # TimestampType cho trường time\n",
    "    StructField(\"id\", StringType(), True),\n",
    "    StructField(\"lon_storm\", DoubleType(), True),\n",
    "    StructField(\"lat_storm\", DoubleType(), True),\n",
    "    StructField(\"amp_storm\", DoubleType(), True),\n",
    "    StructField(\"max_wind_speed\", DoubleType(), True),\n",
    "    StructField(\"area_storm\", DoubleType(), True),\n",
    "    StructField(\"regions_storm\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Tạo DataFrame với schema\n",
    "storms_df = spark.createDataFrame(rows_with_converted_time, schema=schema)\n",
    "\n",
    "# Hiển thị DataFrame\n",
    "storms_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "table_name = \"strong-ward-437213-j6.bigdata_20241.storm_detection_by_pnh\"\n",
    "storms_df.write.format(\"bigquery\") \\\n",
    "    .option(\"table\", table_name) \\\n",
    "    .option(\"writeMethod\", \"direct\") \\\n",
    "    .mode(\"append\") \\\n",
    "    .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "lon_storms, lat_storms, amp_storms, max_wind_speeds, area_storms, regions_storm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Tạo các biến mới từ các danh sách ban đầu\n",
    "lon_storms_array = np.array(lon_storms)\n",
    "lat_storms_array = np.array(lat_storms)\n",
    "amp_storms_array = np.array(amp_storms)\n",
    "wind_speeds_array = np.array(max_wind_speeds)  # Đảm bảo không bị lỗi gõ phím\n",
    "area_storms_array = np.array(area_storms)\n",
    "regions_array = np.array(regions_storm)\n",
    "\n",
    "# In ra kích thước của các biến mới\n",
    "print(lon_storms_array.shape)\n",
    "print(lat_storms_array.shape)\n",
    "print(amp_storms_array.shape)\n",
    "print(wind_speeds_array.shape)\n",
    "print(area_storms_array.shape)\n",
    "print(regions_array.shape)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
