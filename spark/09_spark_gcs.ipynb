{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.conf import SparkConf\n",
    "from pyspark.context import SparkContext\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkConf\n",
    "credentials_location = '/home/phuc_0703/learnDE/data-engineering-zoomcamp/strong-ward-437213-j6-c3ae16d10e5f.json'\n",
    "\n",
    "conf = SparkConf() \\\n",
    "    .setMaster('local[*]') \\\n",
    "    .setAppName('test') \\\n",
    "    .set(\"spark.jars\", \"/home/phuc_0703/learnDE/data-engineering-zoomcamp/05-batch/libs/gcs-connector-hadoop3-2.2.5.jar\") \\\n",
    "    .set(\"spark.hadoop.google.cloud.auth.service.account.enable\", \"true\") \\\n",
    "    .set(\"spark.hadoop.google.cloud.auth.service.account.json.keyfile\", credentials_location)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "sc = SparkContext(conf=conf)\n",
    "\n",
    "hadoop_conf = sc._jsc.hadoopConfiguration()\n",
    "\n",
    "hadoop_conf.set(\"fs.AbstractFileSystem.gs.impl\",  \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS\")\n",
    "hadoop_conf.set(\"fs.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem\")\n",
    "hadoop_conf.set(\"fs.gs.auth.service.account.json.keyfile\", credentials_location)\n",
    "hadoop_conf.set(\"fs.gs.auth.service.account.enable\", \"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .config(conf=sc.getConf()) \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import LongType, StructType, StructField, FloatType, IntegerType, DoubleType\n",
    "from pyspark.sql.functions import col, from_unixtime\n",
    "from pyspark.sql import functions as F\n",
    "schema = StructType([\n",
    "    StructField(\"time\", LongType(), True),\n",
    "    StructField(\"latitude\", DoubleType(), True),\n",
    "    StructField(\"longitude\", DoubleType(), True),\n",
    "    StructField(\"number\", LongType(), True),\n",
    "    StructField(\"step\", LongType(), True),\n",
    "    StructField(\"surface\", DoubleType(), True),\n",
    "    StructField(\"valid_time\", LongType(), True),\n",
    "    StructField(\"u10\", FloatType(), True),\n",
    "    StructField(\"v10\", FloatType(), True),\n",
    "    StructField(\"d2m\", FloatType(), True),\n",
    "    StructField(\"t2m\", FloatType(), True),\n",
    "    StructField(\"msl\", FloatType(), True),\n",
    "    StructField(\"sst\", FloatType(), True),\n",
    "    StructField(\"sp\", FloatType(), True),\n",
    "    StructField(\"tcc\", FloatType(), True),\n",
    "    StructField(\"tciw\", FloatType(), True),\n",
    "    StructField(\"tclw\", FloatType(), True)\n",
    "])\n",
    "\n",
    "\n",
    "df_weather = spark.read.option(\"basePath\", \"gs://weather_bigdata_20241/other_data/year=2024/\") \\\n",
    "                       .schema(schema) \\\n",
    "                       .parquet(\"gs://weather_bigdata_20241/other_data/year=2024/*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# from pyspark.sql.types import LongType, StructType, StructField, FloatType, IntegerType, DoubleType\n",
    "# from pyspark.sql.functions import col, from_unixtime\n",
    "# from pyspark.sql import functions as F\n",
    "# schema = StructType([\n",
    "#     StructField(\"time\", LongType(), True),\n",
    "#     StructField(\"latitude\", DoubleType(), True),\n",
    "#     StructField(\"longitude\", DoubleType(), True),\n",
    "#     StructField(\"number\", LongType(), True),\n",
    "#     StructField(\"step\", LongType(), True),\n",
    "#     StructField(\"surface\", DoubleType(), True),\n",
    "#     # StructField(\"valid_time\", LongType(), True),\n",
    "#     StructField(\"u10\", FloatType(), True),\n",
    "#     StructField(\"v10\", FloatType(), True),\n",
    "#     StructField(\"d2m\", FloatType(), True),\n",
    "#     StructField(\"t2m\", FloatType(), True),\n",
    "#     StructField(\"msl\", FloatType(), True),\n",
    "#     StructField(\"sst\", FloatType(), True),\n",
    "#     StructField(\"sp\", FloatType(), True),\n",
    "#     StructField(\"tcc\", FloatType(), True),\n",
    "#     StructField(\"tciw\", FloatType(), True),\n",
    "#     StructField(\"tclw\", FloatType(), True),\n",
    "#     StructField(\"tp\", FloatType(), True),\n",
    "# ])\n",
    "\n",
    "\n",
    "# df_weather = spark.read.option(\"basePath\", \"gs://weather_bigdata_20241/weather_joined/\") \\\n",
    "#                        .schema(schema) \\\n",
    "#                        .parquet(\"gs://weather_bigdata_20241/weather_joined/*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "    StructField(\"time\", LongType(), True),\n",
    "    StructField(\"latitude\", DoubleType(), True),\n",
    "    StructField(\"longitude\", DoubleType(), True),\n",
    "    StructField(\"number\", LongType(), True),\n",
    "    StructField(\"step\", LongType(), True),\n",
    "    StructField(\"surface\", DoubleType(), True),\n",
    "    StructField(\"valid_time\", LongType(), True),\n",
    "    StructField(\"tp\", FloatType(), True)\n",
    "])\n",
    "\n",
    "\n",
    "df_rain = spark.read.option(\"basePath\", \"gs://weather_bigdata_20241/rain/year=2024\") \\\n",
    "                       .schema(schema) \\\n",
    "                       .parquet(\"gs://weather_bigdata_20241/rain/year=2024/*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "df_rain.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Lọc các bản ghi mà trường \"tp\" không null\n",
    "df_rain_filtered = df_rain.filter(df_rain.tp.isNotNull())\n",
    "\n",
    "# Đếm số lượng bản ghi sau khi lọc\n",
    "# print(f\"Số lượng bản ghi không null ở trường 'tp': {df_rain_filtered.count()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "df_weather.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Thực hiện JOIN với điều kiện\n",
    "df_joined = df_rain_filtered.join(\n",
    "    df_weather,\n",
    "    (df_rain_filtered.latitude == df_weather.latitude) &\n",
    "    (df_rain_filtered.longitude == df_weather.longitude) &\n",
    "    (df_rain_filtered.valid_time == df_weather.time),\n",
    "    \"inner\"  # Chỉ lấy các bản ghi khớp nhau (INNER JOIN)\n",
    ")\n",
    "\n",
    "# Chọn các cột cần thiết và đổi tên nếu cần\n",
    "df_joined = df_joined.select(\n",
    "    df_rain_filtered.latitude.alias(\"latitude\"),\n",
    "    df_rain_filtered.longitude.alias(\"longitude\"),\n",
    "    df_rain_filtered.valid_time.alias(\"time\"),\n",
    "    df_weather.step,\n",
    "    df_weather.surface,\n",
    "    df_weather.t2m,\n",
    "    df_weather.d2m,\n",
    "    df_weather.u10,\n",
    "    df_weather.v10,\n",
    "    df_weather.msl,\n",
    "    df_weather.sst,\n",
    "    df_weather.sp,\n",
    "    df_weather.tcc,\n",
    "    df_weather.tciw,\n",
    "    df_weather.tclw,\n",
    "    df_rain_filtered.tp\n",
    ")\n",
    "\n",
    "# # Đếm số lượng bản ghi\n",
    "# print(df_joined.count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "df_joined.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "df_weather.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "df_rain.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "df_joined = df_joined.withColumn(\"time\", from_unixtime(col(\"time\") / 1_000_000_000).cast(\"timestamp\"))\n",
    "df_joined = df_joined.withColumn(\"d2m\", F.col(\"d2m\") - 273.15) \\\n",
    "                        .withColumn(\"t2m\", F.col(\"t2m\") - 273.15)\n",
    "# df_joined.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "df_joined.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import year, month, dayofmonth\n",
    "df_joined = df_joined \\\n",
    "    .withColumn(\"year\", year(col(\"time\"))) \\\n",
    "    .withColumn(\"month\", month(col(\"time\"))) \\\n",
    "    .withColumn(\"day\", dayofmonth(col(\"time\")))\n",
    "\n",
    "output_path = \"gs://weather_bigdata_20241/weather_all\"\n",
    "\n",
    "df_joined.write \\\n",
    "    .partitionBy(\"year\", \"month\", \"day\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .parquet(output_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "df_weather_main = (\n",
    "    spark.read.option(\"timestampAsString\", \"true\")\n",
    "    .parquet(\"gs://weather_bigdata_20241/weather_all_fixed_partitions/*\")\n",
    ")\n",
    "df_weather_main = df_weather_main.withColumn(\"time\", df_weather_main[\"time\"].cast(\"timestamp\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "df_weather_main.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "df_weather_main.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "time = time.time() - start\n",
    "time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "df_weather_main = df_weather_main.withColumn(\"wind_speed\", F.sqrt(F.col(\"u10\")**2 + F.col(\"v10\")**2))\n",
    "df_weather_main.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "df_weather_main = df_weather_main.withColumn(\"vapor_pressure\", \n",
    "    6.11 * (10 ** (7.5 * F.col(\"d2m\") / (F.col(\"d2m\") + 237.3)))\n",
    ")\n",
    "df_weather_main.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "df_weather_main = df_weather_main.withColumn(\"saturation_vapor_pressure\", \n",
    "    6.11 * (10 ** (7.5 * F.col(\"t2m\") / (F.col(\"t2m\") + 237.3)))\n",
    ")\n",
    "df_weather_main.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "df_weather_main = df_weather_main.withColumn(\"relative_humidity\", \n",
    "    (F.col(\"vapor_pressure\") / F.col(\"saturation_vapor_pressure\")) * 100\n",
    ")\n",
    "df_weather_main.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "df_weather_main = df_weather_main.withColumn(\"apparent_temperature\", \n",
    "    (F.col(\"t2m\")) +\n",
    "    0.33 * F.col(\"vapor_pressure\") - 0.70 * F.col(\"wind_speed\") - 4.00\n",
    ")\n",
    "df_weather_main.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "R = 287.05\n",
    "df_weather_main = df_weather_main.withColumn(\"air_density\", \n",
    "    F.col(\"sp\") / (R * (F.col(\"t2m\") + 273.15)) \n",
    ")\n",
    "df_weather_main.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "df_weather_main = df_weather_main.withColumn(\n",
    "    \"wind_direction\",\n",
    "    F.when(\n",
    "        (180 + (180 / math.pi) * F.atan2(F.col(\"u10\"), F.col(\"v10\"))) < 0,\n",
    "        360 + (180 + (180 / math.pi) * F.atan2(F.col(\"u10\"), F.col(\"v10\")))\n",
    "    ).otherwise(\n",
    "        180 + (180 / math.pi) * F.atan2(F.col(\"u10\"), F.col(\"v10\"))\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Define the latitude and longitude range filters\n",
    "df_weather_main = df_weather_main.filter(\n",
    "    (F.col(\"latitude\") >= 8) & (F.col(\"latitude\") <= 24) &\n",
    "    (F.col(\"longitude\") >= 102) & (F.col(\"longitude\") <= 112)\n",
    ")\n",
    "\n",
    "# Select the date and calculate total precipitation\n",
    "df_weather_main = df_weather_main.groupBy(F.to_date(\"valid_time\").alias(\"date\")) \\\n",
    "    .agg(F.sum(\"tp\").alias(\"total_precipitation\")) \\\n",
    "    .orderBy(\"date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "df_result = df_weather_main.select(\"time\", \"latitude\", \"longitude\", \"wind_speed\", \"vapor_pressure\", \n",
    "                        \"relative_humidity\", \"apparent_temperature\", \"air_density\", \"wind_direction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "df_result.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "df_result = df_result \\\n",
    "    .withColumn(\"year\", year(col(\"time\"))) \\\n",
    "    .withColumn(\"month\", month(col(\"time\"))) \\\n",
    "    .withColumn(\"day\", dayofmonth(col(\"time\")))\n",
    "\n",
    "output_path = \"gs://weather_bigdata_20241/result\"\n",
    "\n",
    "df_result.write \\\n",
    "    .partitionBy(\"year\", \"month\", \"day\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .parquet(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "sc.stop()\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
